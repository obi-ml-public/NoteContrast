{
 "model_name_or_path": "roberta-base",
 "tokenizer_name": "/mnt/<path>/phi/ehr_projects/icd_embeddings/tokenizers/icd10_roberta.json",
 "train_file": "/mnt/<path>/phi/ehr_projects/icd_embeddings/data/icd10_sequences_no_sampling/train.jsonl",
 "validation_file": "/mnt/<path>/phi/ehr_projects/icd_embeddings/data/icd10_sequences_no_sampling/validation_sampled.jsonl",
 "truncation": true,
 "max_seq_length": 514,
 "pad_to_max_length": false,
 "text_column_name": "text",
 "output_dir": "/mnt/<path>/phi/ehr_projects/icd_embeddings/models/v0/no_sampling_trial_2/",
 "logging_dir": "/mnt/<path>/phi/ehr_projects/icd_embeddings/logs/v0/no_sampling_trial_2/",
 "overwrite_output_dir": true,
 "do_train": true,
 "do_eval": true,
 "do_predict": false,
 "report_to": ["tensorboard", "wandb"],
 "run_name": "v0_no_sampling_trial_2",
 "per_device_train_batch_size": 64,
 "per_device_eval_batch_size": 64,
 "gradient_accumulation_steps": 4,
 "logging_strategy": "steps",
 "logging_steps": 500,
 "evaluation_strategy": "steps",
 "eval_steps": 2000,
 "save_strategy": "steps",
 "save_steps": 2000,
 "weight_decay": 0.01,
 "learning_rate": 7e-4,
 "max_steps": 200000,
 "lr_scheduler_type": "linear",
 "warmup_steps": 4000,
 "adam_beta1": 0.9,
 "adam_beta2": 0.999,
 "adam_epsilon": 1e-6,
 "dataloader_drop_last": true,
 "fp16": true,
 "dataloader_num_workers": 32,
 "log_level": "error",
 "position_ids_column_name": "position_ids",
 "token_type_ids_column_name": "token_type_ids",
 "ddp_find_unused_parameters": false,
 "max_grad_norm": 1.0
}