{
 "model_name_or_path": "/mnt/<path>/phi/ehr_projects/note_pretraining/models/mlm/mimic_3/v1_3500/trial_1/checkpoint-3500",
 "train_file": "/mnt/<path>/phi/ehr_projects/note_pretraining/data/mimic_3/train_mlm.jsonl",
 "validation_file": "/mnt/<path>/phi/ehr_projects/note_pretraining/data/mimic_3/validation_mlm.jsonl",
 "max_seq_length": 4096,
 "pad_to_max_length": true,
 "train_on_fly": true,
 "text_column_name": "text",
 "output_dir": "/mnt/<path>/phi/ehr_projects/note_pretraining/models/mlm/mimic_3/v1/trial_1/",
 "logging_dir": "/mnt/<path>/phi/ehr_projects/note_pretraining/logs/mlm/mimic_3/v1/trial_1/",
 "overwrite_output_dir": true,
 "do_train": true,
 "do_eval": true,
 "do_predict": false,
 "report_to": ["tensorboard", "wandb"],
 "run_name": "mlm_mimic_3_v1_trial_1",
 "per_device_train_batch_size": 8,
 "per_device_eval_batch_size": 4,
 "gradient_accumulation_steps": 16,
 "logging_strategy": "steps",
 "logging_steps": 50,
 "evaluation_strategy": "steps",
 "eval_steps": 500,
 "save_strategy": "steps",
 "save_steps": 500,
 "weight_decay": 0.01,
 "learning_rate": 5e-4,
 "max_steps": 30000,
 "lr_scheduler_type": "linear",
 "warmup_steps": 100,
 "adam_beta1": 0.9,
 "adam_beta2": 0.999,
 "adam_epsilon": 1e-6,
 "max_grad_norm": 1.0,
 "dataloader_drop_last": true,
 "fp16": true,
 "gradient_checkpointing": false,
 "dataloader_num_workers": 32,
 "log_level": "error"
}